{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f532cdd-9e23-46c9-aa20-9b4dcd63fd61",
   "metadata": {
    "tags": []
   },
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb926a2f",
   "metadata": {},
   "source": [
    "# My Solution # 1 # Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ecb51-c2f1-4a34-8591-ddec74630593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName='sga')\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1acc6-e849-4b93-877d-89c73e6be618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clickstream = se.read.csv('clickstream.csv', header=True, sep='\\t')\n",
    "\n",
    "# Сheck for errors over (and within) different sessions and users!\n",
    "events = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")\n",
    "# Creating flags for rows that either contain the 'error' substring or follow the row with the 'error' substring.\n",
    "# Utilizing the window function to define the context based on users and sessions, ordered by timestamp.\n",
    "clickstream_w_error_flags = (clickstream\n",
    "                             .withColumn(\"error_flag\", F.max(F.when(f.col(\"event_type\").contains(\"error\"), 'error').otherwise(0)).over(events)))\n",
    "\n",
    "# Creating a new DataFrame by filtering out rows identified as errors.\n",
    "# The DataFrame 'without_errors' is derived from 'clickstream_w_error_flags' and includes only rows where the 'error_flag' is not equal to 1.\n",
    "# This operation effectively removes rows associated with errors, providing a dataset that excludes entries flagged as errors.\n",
    "without_errors = clickstream_w_error_flags.filter(F.col('error_flag') != 'error')\n",
    "\n",
    "# the only way to collect routes in a parallelized way is via collect_list func.\n",
    "# however, we need to deduplicate cases when consecutive rows contain the same 'event_page' value\n",
    "# this is done because internet -> internet is not a valid click transition and should not be considered\n",
    "filtered_w_dup_flags = without_errors.withColumn(\"to_remove\",F.when(F.lag('event_page').over(events) == F.col('event_page'), 'duplicate').otherwise(0))\n",
    "no_duplicate = filtered_w_dup_flags.filter(F.col('to_remove') != 'duplicate')\n",
    "\n",
    "# Generating a concatenated string representing the maximum route per user and session.\n",
    "# The DataFrame 'no_duplicate_routes' undergoes a transformation where a new column 'sorted_route' is created.\n",
    "# This column is populated by concatenating the distinct 'event_page' values within each user and session, ordered by the specified window defined by the 'events'.\n",
    "# The resulting 'sorted_route' column provides a string representation of the maximum route for each unique combination of user and session.\n",
    "no_duplicate_routes = no_duplicate.withColumn('sorted_route', F.concat_ws(\"-\", F.collect_list('event_page').over(events)))\n",
    "\n",
    "# Identifying all maximum routes for each user session.\n",
    "# The DataFrame 'no_duplicate_routes' is grouped by 'user_id' and 'session_id'.\n",
    "# The aggregation operation is applied to find the maximum value of the 'sorted_route' column within each group, and the resulting column is named 'route'.\n",
    "# The DataFrame 'max_routes' contains information about the maximum route for every distinct user session combination.\n",
    "max_routes = no_duplicate_routes\\\n",
    "          .groupby('user_id', 'session_id')\\\n",
    "          .agg(F.last('sorted_route').alias('route'))\n",
    "\n",
    "# ANSWER\n",
    "ANSWER_df = max_routes\\\n",
    "                .groupby('route').agg(f.count('*').alias('count'))\\\n",
    "                .orderBy(F.desc('count'))\\\n",
    "                .limit(30)\n",
    "\n",
    "ANSWER_df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee27fd4",
   "metadata": {},
   "source": [
    "# My Solution # 2 # Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f4b2f-e626-4a2a-9f9e-dd92440ff6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clickstream.createOrReplaceTempView('clickstream')\n",
    "\n",
    "ANSWER_sql = se.sql(\"\"\"\n",
    "WITH error_flagged AS (\n",
    "    SELECT \n",
    "        user_id, \n",
    "        session_id, \n",
    "        event_page, \n",
    "        timestamp,\n",
    "        MAX(CASE WHEN event_type LIKE '%error%' THEN NULL ELSE FALSE END) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) AS error\n",
    "    FROM \n",
    "        clickstream),\n",
    "dup_flagged AS (\n",
    "    SELECT \n",
    "        user_id, \n",
    "        session_id,\n",
    "        event_page,\n",
    "        timestamp,\n",
    "        CASE WHEN LAG(event_page) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) = event_page THEN NULL ELSE FALSE END AS dupl\n",
    "    FROM \n",
    "        error_flagged\n",
    "    WHERE\n",
    "        error IS NOT NULL),\n",
    "collected_routes AS (\n",
    "    SELECT \n",
    "        user_id, \n",
    "        session_id,\n",
    "        CONCAT_WS('-', COLLECT_LIST(event_page) OVER (PARTITION BY user_id, session_id ORDER BY timestamp)) AS route\n",
    "    FROM \n",
    "        dup_flagged\n",
    "    WHERE \n",
    "        dupl IS NOT NULL\n",
    "),\n",
    "routes AS (\n",
    "    SELECT\n",
    "        user_id, \n",
    "        session_id,\n",
    "        MAX(route) AS route\n",
    "    FROM \n",
    "        collected_routes\n",
    "    GROUP BY\n",
    "        user_id, session_id)\n",
    "\n",
    "SELECT route, COUNT(*) as count FROM routes\n",
    "GROUP BY route\n",
    "ORDER BY count DESC LIMIT 30\n",
    "\"\"\")\n",
    "\n",
    "ANSWER_sql.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598cac3",
   "metadata": {},
   "source": [
    "# My Solution # 3 # Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e1523-8d09-436f-9edf-c461a0eadc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%markdown\n",
    "# RDD solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9a0e3-e02a-4137-8978-9dad648805d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(row):\n",
    "    return [x for i, x in enumerate(row) if i == 0 or row[i] != row[i-1]]\n",
    "\n",
    "# Identifying rows containing errors in the 'event_type'.\n",
    "errors = clickstream.rdd.filter(lambda row: 'error' in row.event_type).map(lambda row: ((row.user_id, row.session_id), row.timestamp)).reduceByKey(lambda x, y: min(x, y))\n",
    "\n",
    "# Filtering out rows associated with errors or those occurring after an error.\n",
    "no_errors = clickstream.rdd.map(lambda row: ((row.user_id, row.session_id), row)).leftOuterJoin(errors).filter(lambda row: row[1][1] is None or row[1][0].timestamp < row[1][1]).map(lambda row: row[1][0])\n",
    "\n",
    "# Mapping rows to key-value pairs where keys are (user_id, session_id) and values are event_page.\n",
    "best_routes = no_errors.map(lambda row: ((row.user_id, row.session_id), row.event_page)).groupByKey().map(lambda row: (row[0], list(row[1])))\n",
    "\n",
    "\n",
    "# Applying the remove_dupl function to the list of event_pages in each group.\n",
    "best_routes = best_routes.map(lambda row: (row[0], '-'.join(remove_duplicates(row[1]))))\n",
    "\n",
    "# Creating an RDD 'ANSWERS_rdd' with counts of each unique route, sorted in descending order.\n",
    "ANSWERS_rdd = best_routes.map(lambda row: (row[1], 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda row: row[1], ascending=False).take(30)\n",
    "\n",
    "# The resulting 'ANSWERS_rdd' contains the top 30 routes along with their respective counts.\n",
    "ANSWERS_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3479c98-b885-4d69-a170-85518f5cd5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "routes_pd = ANSWER_df.toPandas()\n",
    "routes_pd.to_csv('ANSWERS_df.csv', sep='\\t', index=False)\n",
    "\n",
    "routes_pd = ANSWER_sql.toPandas()\n",
    "routes_pd.to_csv('ANSWERS_sql.csv', sep='\\t', index=False)\n",
    "\n",
    "with open('ANSWERS_rdd.csv', 'w') as f:\n",
    "    f.write(f'route\\tcount\\n')\n",
    "    for route, count in ANSWERS_rdd:\n",
    "        f.write(f'{route}\\t{count}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a775aa-bd9e-49d9-aae9-5dd5fac7d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "se.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
